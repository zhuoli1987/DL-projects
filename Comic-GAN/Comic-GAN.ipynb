{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manga DCGAN\n",
    "\n",
    "This notebook is an expriment of using DC-GAN (Deep Convolutional Generative Adverserial Network) to generate comic/manga characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    input_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')\n",
    "    input_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    return input_real, input_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_image(image_path, width, height, mode):\n",
    "    \"\"\"\n",
    "    Read image from image_path\n",
    "    :param image_path: Path of image\n",
    "    :param width: Width of image\n",
    "    :param height: Height of image\n",
    "    :param mode: Mode of image\n",
    "    :return: Image data\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if image.size == (width, height):\n",
    "        return np.array(image.convert(mode))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_batch(image_files, width, height, mode):\n",
    "    data_batch = np.array(\n",
    "        [get_image(file, width, height, mode) for file in image_files]).astype(np.float32)\n",
    "    \n",
    "    # Make sure the images are in 4 dimensions\n",
    "    if len(data_batch.shape) < 4:\n",
    "        data_batch = data_batch.reshape(data_batch.shape + (1,))\n",
    "        \n",
    "    return data_batch\n",
    "\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    # scale to (0, 1)\n",
    "    x = ((x - x.min())/(255 - x.min()))\n",
    "    \n",
    "    # scale to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data_files, scale_func=None):\n",
    "        \"\"\"\n",
    "        Initialize the class\n",
    "        :param data_files: List of files in the database\n",
    "        :param scale_func: Scale function\n",
    "        \"\"\"\n",
    "        IMAGE_WIDTH = 96\n",
    "        IMAGE_HEIGHT = 96\n",
    "        image_channels = 3\n",
    "        \n",
    "        if scale_func is None:\n",
    "            self.scaler = scale\n",
    "        else:\n",
    "            self.scaler = scale_func\n",
    "            \n",
    "        self.image_mode = 'RGB'\n",
    "        self.data_files = data_files\n",
    "        self.shape = len(data_files), IMAGE_WIDTH, IMAGE_HEIGHT, image_channels\n",
    "        \n",
    "    def get_batches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate batches\n",
    "        :param batch_size: Batch size\n",
    "        :return Batches of data\n",
    "        \"\"\"\n",
    "        current_index = 0\n",
    "        while current_index + batch_size <= self.shape[0]:\n",
    "            data_batch = get_batch(\n",
    "                self.data_files[current_index:current_index + batch_size],\n",
    "                *self.shape[1:3],\n",
    "                self.image_mode)\n",
    "\n",
    "            current_index += batch_size\n",
    "            \n",
    "            yield self.scaler(data_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, output_dim, reuse=False, alpha=0.2, training=True):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        \n",
    "        # First fully connected layer\n",
    "        layer1 = tf.layers.dense(z, 6*6*1024)\n",
    "        layer1 = tf.reshape(layer1, (-1, 6, 6, 1024))\n",
    "        layer1 = tf.layers.batch_normalization(layer1, training=training)\n",
    "        layer1 = tf.maximum(alpha * layer1, layer1) # Leaky ReLU\n",
    "        \n",
    "        # First conv layer\n",
    "        layer2 = tf.layers.conv2d_transpose(layer1, 512, 5, strides=2, padding='same')\n",
    "        layer2 = tf.layers.batch_normalization(layer2, training=training)\n",
    "        layer2 = tf.maximum(alpha * layer2, layer2) # Leaky ReLU\n",
    "        \n",
    "        # Second conv layer\n",
    "        layer3 = tf.layers.conv2d_transpose(layer2, 256, 5, strides=2, padding='same')\n",
    "        layer3 = tf.layers.batch_normalization(layer3, training=training)\n",
    "        layer3 = tf.maximum(alpha * layer3, layer3) # Leaky ReLU\n",
    "        \n",
    "        # Third conv layer\n",
    "        layer4 = tf.layers.conv2d_transpose(layer3, 128, 5, strides=2, padding='same')\n",
    "        layer4 = tf.layers.batch_normalization(layer4, training=training)\n",
    "        layer4 = tf.maximum(alpha * layer4, layer4) # Leaky ReLU\n",
    "        \n",
    "        # Output layer, 96x96x3\n",
    "        logits = tf.layers.conv2d_transpose(layer4, output_dim, 5, strides=2, padding='same')\n",
    "        \n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, reuse=False, alpha=0.2):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        # Input is 96x96x3\n",
    "        layer1 = tf.layers.conv2d(x, 128, 5, strides=2, padding='same')\n",
    "        relu1 = tf.maximum(alpha * layer1, layer1)\n",
    "\n",
    "        # 48x48x128\n",
    "        layer2 = tf.layers.conv2d(relu1, 256, 5, strides=2, padding='same')\n",
    "        bn2 = tf.layers.batch_normalization(layer2, training=True)\n",
    "        relu2 = tf.maximum(alpha * bn2, bn2)\n",
    "\n",
    "        # 24x24x256\n",
    "        layer3 = tf.layers.conv2d_transpose(relu2, 512, 5, strides=2, padding='same')\n",
    "        bn3 = tf.layers.batch_normalization(layer3, training=True)\n",
    "        relu3 = tf.maximum(alpha * bn3, bn3)\n",
    "\n",
    "        # 12x12x512\n",
    "        layer4 = tf.layers.conv2d_transpose(relu3, 1024, 5, strides=2, padding='same')\n",
    "        bn4 = tf.layers.batch_normalization(layer4, training=True)\n",
    "        relu4 = tf.maximum(alpha * bn4, bn4)\n",
    "\n",
    "        # 6x6x1024\n",
    "        flat = tf.reshape(relu4, (-1, 6*6*1024))\n",
    "        logits = tf.layers.dense(flat, 1)\n",
    "        out = tf.sigmoid(logits)\n",
    "\n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, output_dim, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param input_real: Images from the real dataset\n",
    "    :param input_z: Z input\n",
    "    :param out_channel_dim: The number of channels in the output image\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    g_model = generator(input_z, output_dim, alpha=alpha)\n",
    "    d_model_real, d_logits_real = discriminator(input_real, alpha=alpha)\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True, alpha=alpha)\n",
    "    \n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_model_fake)))\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_model_real)))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_model_fake)))\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator loss Tensor\n",
    "    :param g_loss: Generator loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    \n",
    "    # Optimize, Using Adam optimizer\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, real_size, z_size, learning_rate, alpha=0.2, beta1=0.5):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Create input place holders\n",
    "        self.input_real, self.input_z = model_inputs(real_size, z_size)\n",
    "        \n",
    "        # Get the model losses\n",
    "        self.d_loss, self.g_loss = model_loss(self.input_real, self.input_z, real_size[2], alpha=alpha)\n",
    "        \n",
    "        # Get the optimized parameters\n",
    "        self.d_opt, self.g_opt = model_opt(self.d_loss, self.g_loss, learning_rate, beta1=beta1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper method to visualize the generated outout\n",
    "def view_samples(epoch, samples, nrows, ncols, figsize=(5,5)):\n",
    "    fig, axes = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols, \n",
    "                             sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flattern(), samples[epoch]):\n",
    "        ax.axis('off')\n",
    "        img = ((img - img.min())*255 / (img.max() - img.min())).astype(np.uint8)\n",
    "        ax.set_adjustable('box-forced')\n",
    "        im = ax.imshow(img, aspect='equal')\n",
    "    \n",
    "    # No gap between subplots\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs, batch_size, print_every=10, show_every=100, figsize=(5,5)):\n",
    "    saver = tf.train.Saver() # Saver used to save the checkpoints\n",
    "    samples, losses = [], [] # Outputs\n",
    "    \n",
    "    sample_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "    \n",
    "    steps = 0 # This variable is for showing the generator images\n",
    "    \n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        # Initialize the variables, this is a \n",
    "        # standard tensorflow operation\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Loop through epochs...\n",
    "        for e in range(epochs):\n",
    "            for x in dataset.get_batches(batch_size):\n",
    "                steps += 1\n",
    "                \n",
    "                # Sample random noise for G\n",
    "                batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "                \n",
    "                # Run optimizers\n",
    "                _ = print(sess.run(model.d_opt, feed_dict={model.input_real: x, model.input_z: batch_z}))\n",
    "                _ = sess.run(model.g_opt, feed_dict={model.input_z: batch_z, model.input_real: x})\n",
    "                \n",
    "                # Display options\n",
    "                if steps % print_every == 0:\n",
    "                    # At the end of each epoch, get the losses and print them out\n",
    "                    train_loss_d = model.d_loss.eval({model.input_z:batch_z, model.input_real:x})\n",
    "                    train_loss_g = model.g_loss.eval({model.input_z:batch_z})\n",
    "                    print(\"Epoch {}/{}...\".format(e+1, epochs),\n",
    "                          \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "                          \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "                    \n",
    "                    # Save losses for later view\n",
    "                    losses.append((train_loss_d, train_loss_g))\n",
    "                    \n",
    "                if steps % show_every == 0:\n",
    "                    comic_gen = session.run(\n",
    "                                    generator(model.input_z, 3, reuse=True, training=False),\n",
    "                                    feed_dict={model.input_z: sample_z})\n",
    "                    # Display generated samples\n",
    "                    samples.append(comic_gen)\n",
    "                    _ = view_samples(-1, samples, 5, 5, figsize=figsize)\n",
    "                    plt.show()\n",
    "                \n",
    "        saver.save(sess, './checkpoints/generator.ckpt')  \n",
    "   \n",
    "    with open('samples.pkl', 'wb') as f:\n",
    "        pkl.dump(samples, f)\n",
    "        \n",
    "    return losses, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_size = (96,96,3)\n",
    "z_size = 200\n",
    "learning_rate = 0.0002\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "alpha = 0.2\n",
    "beta1 = 0.5\n",
    "\n",
    "# Create the network\n",
    "model = GAN(real_size, z_size, learning_rate, alpha=alpha, beta1=beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_folder_path = os.getcwd() + '/data'\n",
    "\n",
    "dataset = Dataset(glob(os.path.join(data_folder_path, '**/*.jpg'), recursive=True))\n",
    "                  \n",
    "# Training the network\n",
    "losses, samples = train(model, dataset, epochs, batch_size, figsize=(24,24))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
