{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manga DCGAN\n",
    "\n",
    "This notebook is an expriment of using DC-GAN (Deep Convolutional Generative Adverserial Network) to generate comic/manga characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file checkpoints already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    input_real = tf.placeholder(tf.float32, (None, *real_dim), name='input_real')\n",
    "    input_z = tf.placeholder(tf.float32, (None, z_dim), name='input_z')\n",
    "    return input_real, input_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(image_path, width, height, mode):\n",
    "    \"\"\"\n",
    "    Read image from image_path\n",
    "    :param image_path: Path of image\n",
    "    :param width: Width of image\n",
    "    :param height: Height of image\n",
    "    :param mode: Mode of image\n",
    "    :return: Image data\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if image.size != (width, height):\n",
    "        # Resize image\n",
    "        image = image.resize((width, height))\n",
    "    \n",
    "    return np.array(image.convert(mode))\n",
    "\n",
    "\n",
    "def get_batch(image_files, width, height, mode):\n",
    "    data_batch = np.array(\n",
    "        [get_image(file, width, height, mode) for file in image_files]).astype(np.float32)\n",
    "    \n",
    "    # Make sure the images are in 4 dimensions\n",
    "    if len(data_batch.shape) < 4:\n",
    "        data_batch = data_batch.reshape(data_batch.shape + (1,))\n",
    "        \n",
    "    return data_batch\n",
    "\n",
    "def scale(x, feature_range=(-1, 1)):\n",
    "    # scale to (0, 1)\n",
    "    x = ((x - x.min())/(255 - x.min()))\n",
    "    \n",
    "    # scale to feature_range\n",
    "    min, max = feature_range\n",
    "    x = x * (max - min) + min\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, data_files, image_width=96, image_height=96, scale_func=None):\n",
    "        \"\"\"\n",
    "        Initialize the class\n",
    "        :param data_files: List of files in the database\n",
    "        :param\n",
    "        :param\n",
    "        :param scale_func: Scale function\n",
    "        \"\"\"\n",
    "        image_channels = 3\n",
    "        \n",
    "        if scale_func is None:\n",
    "            self.scaler = scale\n",
    "        else:\n",
    "            self.scaler = scale_func\n",
    "            \n",
    "        self.image_mode = 'RGB'\n",
    "        self.data_files = data_files\n",
    "        self.shape = len(data_files), image_width, image_height, image_channels\n",
    "        \n",
    "    def get_batches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate batches\n",
    "        :param batch_size: Batch size\n",
    "        :return Batches of data\n",
    "        \"\"\"\n",
    "        current_index = 0\n",
    "        while current_index + batch_size <= self.shape[0]:\n",
    "            data_batch = get_batch(\n",
    "                self.data_files[current_index:current_index + batch_size],\n",
    "                *self.shape[1:3],\n",
    "                self.image_mode)\n",
    "\n",
    "            current_index += batch_size\n",
    "            \n",
    "            yield self.scaler(data_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z, image_dim, output_dim, reuse=False, alpha=0.2, training=True):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        \n",
    "        # First fully connected layer\n",
    "        layer1 = tf.layers.dense(z, image_dim*image_dim*1024)\n",
    "        layer1 = tf.reshape(layer1, (-1, image_dim, image_dim, 1024))\n",
    "        layer1 = tf.layers.batch_normalization(layer1, training=training)\n",
    "        layer1 = tf.maximum(alpha * layer1, layer1) # Leaky ReLU\n",
    "        \n",
    "        # First conv layer\n",
    "        layer2 = tf.layers.conv2d_transpose(layer1, 512, 5, strides=2, padding='same')\n",
    "        layer2 = tf.layers.batch_normalization(layer2, training=training)\n",
    "        layer2 = tf.maximum(alpha * layer2, layer2) # Leaky ReLU\n",
    "        \n",
    "        # Second conv layer\n",
    "        layer3 = tf.layers.conv2d_transpose(layer2, 256, 5, strides=2, padding='same')\n",
    "        layer3 = tf.layers.batch_normalization(layer3, training=training)\n",
    "        layer3 = tf.maximum(alpha * layer3, layer3) # Leaky ReLU\n",
    "        \n",
    "        # Third conv layer\n",
    "        layer4 = tf.layers.conv2d_transpose(layer3, 128, 5, strides=2, padding='same')\n",
    "        layer4 = tf.layers.batch_normalization(layer4, training=training)\n",
    "        layer4 = tf.maximum(alpha * layer4, layer4) # Leaky ReLU\n",
    "        \n",
    "        # Output layer, image_dimximage_dimx3\n",
    "        logits = tf.layers.conv2d_transpose(layer4, output_dim, 5, strides=2, padding='same')\n",
    "        \n",
    "        out = tf.tanh(logits)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x, image_dim, reuse=False, alpha=0.2):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        \n",
    "        # Input is 96x96x3\n",
    "        layer1 = tf.layers.conv2d(x, 128, 5, strides=2, padding='same')\n",
    "        relu1 = tf.maximum(alpha * layer1, layer1)\n",
    "\n",
    "        # 48x48x128\n",
    "        layer2 = tf.layers.conv2d(relu1, 256, 5, strides=2, padding='same')\n",
    "        bn2 = tf.layers.batch_normalization(layer2, training=True)\n",
    "        relu2 = tf.maximum(alpha * bn2, bn2)\n",
    "\n",
    "        # 24x24x256\n",
    "        layer3 = tf.layers.conv2d_transpose(relu2, 512, 5, strides=2, padding='same')\n",
    "        bn3 = tf.layers.batch_normalization(layer3, training=True)\n",
    "        relu3 = tf.maximum(alpha * bn3, bn3)\n",
    "\n",
    "        # 12x12x512\n",
    "        layer4 = tf.layers.conv2d_transpose(relu3, 1024, 5, strides=2, padding='same')\n",
    "        bn4 = tf.layers.batch_normalization(layer4, training=True)\n",
    "        relu4 = tf.maximum(alpha * bn4, bn4)\n",
    "\n",
    "        # 6x6x1024\n",
    "        flat = tf.reshape(relu4, (-1, image_dim*image_dim*1024))\n",
    "        logits = tf.layers.dense(flat, 1)\n",
    "        out = tf.sigmoid(logits)\n",
    "\n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_loss(input_real, image_dim, input_z, output_dim, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param input_real: Images from the real dataset\n",
    "    :param input_z: Z input\n",
    "    :param out_channel_dim: The number of channels in the output image\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    g_model = generator(input_z, image_dim, output_dim, alpha=alpha)\n",
    "    d_model_real, d_logits_real = discriminator(input_real, image_dim, alpha=alpha)\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, image_dim, reuse=True, alpha=alpha)\n",
    "    \n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.ones_like(d_model_fake)))\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real, labels=tf.ones_like(d_model_real)))\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, labels=tf.zeros_like(d_model_fake)))\n",
    "    \n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    \n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_opt(d_loss, g_loss, learning_rate, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator loss Tensor\n",
    "    :param g_loss: Generator loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get weights and bias to update\n",
    "    t_vars = tf.trainable_variables()\n",
    "    d_vars = [var for var in t_vars if var.name.startswith('discriminator')]\n",
    "    g_vars = [var for var in t_vars if var.name.startswith('generator')]\n",
    "    \n",
    "    # Optimize, Using Adam optimizer\n",
    "    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN:\n",
    "    def __init__(self, real_size, z_size, learning_rate, image_size, alpha=0.2, beta1=0.5):\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Image size\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        # Create input place holders\n",
    "        self.input_real, self.input_z = model_inputs(real_size, z_size)\n",
    "        \n",
    "        # Get the model losses\n",
    "        self.d_loss, self.g_loss = model_loss(self.input_real, self.image_size, self.input_z, real_size[2], alpha=alpha)\n",
    "        \n",
    "        # Get the optimized parameters\n",
    "        self.d_opt, self.g_opt = model_opt(self.d_loss, self.g_loss, learning_rate, beta1=beta1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to visualize the generated outout\n",
    "def view_samples(epoch, samples, nrows, ncols, figsize=(5,5)):\n",
    "    fig, axes = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols, \n",
    "                             sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        ax.axis('off')\n",
    "        img = ((img - img.min())*255 / (img.max() - img.min())).astype(np.uint8)\n",
    "        ax.set_adjustable('box-forced')\n",
    "        im = ax.imshow(img, aspect='equal')\n",
    "    \n",
    "    # No gap between subplots\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset, epochs, batch_size, print_every=10, show_every=100, figsize=(5,5)):\n",
    "    saver = tf.train.Saver() # Saver used to save the checkpoints\n",
    "    samples, losses = [], [] # Outputs\n",
    "    \n",
    "    sample_z = np.random.uniform(-1, 1, size=(9, z_size)) # Generate 9 images\n",
    "    \n",
    "    steps = 0 # This variable is for showing the generator images\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    with tf.Session(config=config) as sess:\n",
    "        # Initialize the variables, this is a \n",
    "        # standard tensorflow operation\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Loop through epochs...\n",
    "        for e in range(epochs):\n",
    "            for x in dataset.get_batches(batch_size):\n",
    "                steps += 1\n",
    "                \n",
    "                # Sample random noise for G\n",
    "                batch_z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "                \n",
    "                # Run optimizers\n",
    "                _ = sess.run(model.d_opt, feed_dict={model.input_real: x, model.input_z: batch_z})\n",
    "                _ = sess.run(model.g_opt, feed_dict={model.input_z: batch_z, model.input_real: x})\n",
    "                \n",
    "                # Display options\n",
    "                if steps % print_every == 0:\n",
    "                    # At the end of each epoch, get the losses and print them out\n",
    "                    train_loss_d = model.d_loss.eval({model.input_z:batch_z, model.input_real:x})\n",
    "                    train_loss_g = model.g_loss.eval({model.input_z:batch_z})\n",
    "                    now_time = datetime.datetime.now()\n",
    "\n",
    "                    print(now_time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                          \"Epoch {}/{}...\".format(e+1, epochs),\n",
    "                          \"Discriminator Loss: {:.4f}...\".format(train_loss_d),\n",
    "                          \"Generator Loss: {:.4f}\".format(train_loss_g))\n",
    "                    \n",
    "                    # Save losses for later view\n",
    "                    losses.append((train_loss_d, train_loss_g))\n",
    "                    \n",
    "                if steps % show_every == 0:\n",
    "                    comic_gen = sess.run(\n",
    "                                    generator(model.input_z, model.image_size, 3, reuse=True, training=False),\n",
    "                                    feed_dict={model.input_z: sample_z})\n",
    "                    # Display generated samples\n",
    "                    samples.append(comic_gen)\n",
    "                    _ = view_samples(-1, samples, 3, 3, figsize=figsize)\n",
    "                    plt.show()\n",
    "                    \n",
    "            saver.save(sess, './checkpoints/' + 'generator_' + 'epoch_{}'.format(e+1)+ '.ckpt')  \n",
    "   \n",
    "    with open('samples.pkl', 'wb') as f:\n",
    "        pkl.dump(samples, f)\n",
    "        \n",
    "    return losses, samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = 64\n",
    "image_size = int(image_dim / 16)\n",
    "real_size = (image_dim, image_dim, 3)\n",
    "z_size = 100\n",
    "learning_rate = 0.0002\n",
    "batch_size = 25\n",
    "epochs = 100\n",
    "alpha = 0.2\n",
    "beta1 = 0.5\n",
    "\n",
    "# Create the network\n",
    "model = GAN(real_size, z_size, learning_rate, image_size, alpha=alpha, beta1=beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = os.getcwd() + '/data'\n",
    "\n",
    "dataset = Dataset(glob(os.path.join(data_folder_path, '**/*.jpg'), recursive=True), \n",
    "                  image_width=image_dim, image_height=image_dim)\n",
    "                  \n",
    "# Training the network\n",
    "losses, samples = train(model, dataset, epochs, batch_size, print_every=100, show_every=1000, figsize=(8,8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
